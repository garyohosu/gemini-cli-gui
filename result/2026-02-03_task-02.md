# Result (2026-02-03) â€” Technical Verification (Latency Focus)

## Summary
- Verified the current implementation is a Node HTTP server that loads `@google/gemini-cli-core` at startup, but still executes prompts via the `gemini` CLI subprocess.
- This means the core is warmed in memory, yet each prompt pays subprocess startup cost, so the 90-second cold-start problem is not eliminated for actual requests.

## Static Findings
- `server/gemini_server.js` loads `@google/gemini-cli-core` during startup (`initialize()`), but `/prompt` uses `spawn(geminiCmd, ['-p', prompt, '-o', 'json'])` instead of the core API.
- Path to core is hard-coded to `%APPDATA%\\npm\\node_modules\\@google\\gemini-cli\\node_modules\\@google\\gemini-cli-core`. This can break for:
  - Portable installs or non-global npm installs
  - Packaged distributions (PyInstaller + bundled Node)
  - Non-Windows or custom `npm prefix`

## Attempted Runtime Check
- Tried to launch the server and probe `/health`, but the command was blocked by policy in this environment, so no runtime timing data was collected.

## Risks to the 90s Issue
- Because `/prompt` still spawns the CLI per request, the cold-start cost likely remains (especially if `gemini` itself initializes Node, loads config, and performs auth on first run).
- The in-memory core loading currently has no effect on request latency.

## Next Verification Steps (when runtime is allowed)
1. Measure server startup time to `/health` response.
2. Measure first `/prompt` request latency vs subsequent requests.
3. Compare subprocess-based `/prompt` with a core-API-based `/prompt` (if implemented).
